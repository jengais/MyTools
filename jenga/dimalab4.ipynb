{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7706875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33/33 [==============================] - 9s 232ms/step - loss: 0.6322 - accuracy: 0.6385\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 8s 250ms/step - loss: 0.2884 - accuracy: 0.8955\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 8s 252ms/step - loss: 0.0723 - accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 9s 264ms/step - loss: 0.0152 - accuracy: 0.9967\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 9s 268ms/step - loss: 0.0032 - accuracy: 0.9997\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           1071168   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,145,537\n",
      "Trainable params: 1,145,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "104/104 [==============================] - 2s 14ms/step - loss: 4.7266e-04 - accuracy: 1.0000\n",
      "Test Loss: 0.00047265688772313297\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"model1 = Sequential()\\nmodel1.add(Embedding(vocab_size, embedding_dim, input_length=80))\\nmodel1.add(GRU(50,dropout=0.5))\\n#model.add(Dense(10, activation='relu'))\\nmodel1.add(Dense(1, activation='sigmoid'))\\nmodel1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ex4.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1ri7bpG9UVug0YaVmFfXrqnj7hhA_24Fb\n",
    "\"\"\"\n",
    "#Dmitry Korkin 336377429\n",
    "#Shmuel Atias 300987443\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "#from google.colab import files\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM,GRU, Bidirectional\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#uploaded = files.upload()\n",
    "train_data = pd.read_csv(\"training_ex4_dl2021b.csv\")\n",
    "#train_data= pd.read_csv(io.StringIO(uploaded['training_ex4_dl2021b.csv'].decode('utf-8')))\n",
    "#test_data = pd.read_csv(\"test_ex3_dl2021b.csv\")\n",
    "#uploaded = files.upload()\n",
    "test_data = pd.read_csv(\"test_ex4_dl2021b.csv\")\n",
    "#test_data= pd.read_csv(io.StringIO(uploaded['test_ex4_dl2021b.csv'].decode('utf-8')))\n",
    "\n",
    "\n",
    "# df = pd.read_csv('sentences1138final.csv')\n",
    "# #df = df[:600]\n",
    "# df\n",
    "\n",
    "sentences = train_data['sentence'].values\n",
    "labels = train_data['label'].values\n",
    "sentences_test=test_data['sentence'].values\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(sentences, labels,test_size=0.25,random_state=1000)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "#tokenizer.fit_on_texts(x_train)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# X_train = tokenizer.texts_to_sequences(x_train)\n",
    "# X_test = tokenizer.texts_to_sequences(x_test)\n",
    "X_train = tokenizer.texts_to_sequences(sentences)\n",
    "X_test=tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "#print(sentences)\n",
    "#print(tokenizer.word_index)\n",
    "#print()\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=200)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=200)\n",
    "#print(X_train)\n",
    "\n",
    "#embedding_dim = 64 4.2\n",
    "embedding_dim = 64\n",
    "\n",
    "model = Sequential() #4.2\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=200))\n",
    "model.add(Bidirectional(LSTM(64,  return_sequences=False)))\n",
    "#model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) #4.2\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_dim, input_length=200))\n",
    "# model.add(Bidirectional(LSTM(64,  return_sequences=True)))\n",
    "# model.add(Bidirectional(LSTM(32)))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#model.fit(X_train, y_train,epochs=5,batch_size=10)\n",
    "##model.fit(X_train, labels,epochs=5,batch_size=10) 4.2\n",
    "model.fit(X_train, labels,epochs=5,batch_size=100)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#pred=model.predict_classes(X_test)\n",
    "\n",
    "predict_x=model.predict(X_test) \n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "\n",
    "#print(metrics.accuracy_score(y_test,pred))\n",
    "#print(metrics.accuracy_score(y_test,classes_x))\n",
    "\n",
    "df = pd.DataFrame({'id': test_data['id'],\n",
    "                   'label':[int(x) for x in classes_x],})\n",
    "\n",
    "#df.to_csv('out.csv',index=False)\n",
    "#files.download('out.csv')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "test_loss, test_acc = model.evaluate(X_train, labels)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "'''model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_dim, input_length=80))\n",
    "model1.add(GRU(50,dropout=0.5))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])'''\n",
    "\n",
    "#model1.fit(X_train, y_train,epochs=10,batch_size=10)\n",
    "\n",
    "#pred=model1.predict_classes(X_test)\n",
    "\n",
    "#print(metrics.accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c224d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = (model.predict_classes(numpy.array(X_test)) > 0.5).astype(\"int32\")\n",
    "# test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61227c10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 3300\n  y sizes: 2475\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy)\n\u001b[0;32m      3\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(np\u001b[38;5;241m.\u001b[39marray(X_test), y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1653\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1649\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1650\u001b[0m       label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1651\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   1652\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3300\n  y sizes: 2475\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(np.array(X_train), y_train, verbose=False)\n",
    "print('Train Accuracy: ', accuracy)\n",
    "loss, accuracy = model.evaluate(np.array(X_test), y_test, verbose=False)\n",
    "print('Test Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012a2f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9980950e-01],\n",
       "       [1.1137217e-02],\n",
       "       [9.9999160e-01],\n",
       "       ...,\n",
       "       [8.5229057e-01],\n",
       "       [2.1962523e-03],\n",
       "       [6.6048764e-05]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb0c7ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab43e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbd0239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n",
      "1341\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d659fa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3300, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1665313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
