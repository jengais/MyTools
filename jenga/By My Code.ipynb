{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from datetime import datetime \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(c,d) = P(c|d)P(d) =P(d|c)P(c)\n",
    "##### c - category\n",
    "##### d - document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(c|d) =(  P(d|c) P(c)  ) / P(d)\n",
    "\n",
    "##### c - category\n",
    "##### d - document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(c|d) = P(d|c) P(c)   \n",
    "\n",
    "##### c - category\n",
    "##### d - document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(x1|c):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.misc', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'text': list(newsgroups_train.data) ,'label': list(newsgroups_train.target) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','') \n",
    "df['text'] = df['text'].str.replace('[^a-z ]','') \n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multNom(df_train,df_test,data_name='text',target_name='label'):\n",
    "    df_train=df_train.fillna(\"\")\n",
    "    df_test=df_test.fillna(\"\")\n",
    "    start_time = datetime.now()\n",
    "    label_dict={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "    tfidf_matrix = tf.fit_transform(df_train[data_name])\n",
    "    wordDict=tf.vocabulary_\n",
    "    \n",
    "    \n",
    "    new_df=pd.DataFrame( [[float(0)]*len(wordDict.keys())]*len(set(df_train[target_name].values)), index=set(df_train[target_name].values) )\n",
    "    new_df.columns = wordDict.keys()\n",
    "    \n",
    "    P_Cj_dict={}\n",
    "    \n",
    "    for category in set(df_train[target_name].values):\n",
    "        \n",
    "        category_df=df_train[df_train[target_name] == category]\n",
    "        \n",
    "        tf_1 = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "        #tfidf_matrix = tf_1.fit_transform(category_df[data_name])\n",
    "        #wordDict=tf-1.vocabulary_\n",
    "        \n",
    "        \n",
    "\n",
    "        ################# CREATE ########################\n",
    "        \n",
    "        P_Cj_dict[category]= len(df_train[target_name].values)/len(df_train[data_name])\n",
    "        \n",
    "        for text in category_df[data_name]:\n",
    "            word_set=set(text.split())\n",
    "            #tfidf_matrix = tf_1.fit_transform(text)\n",
    "            #word_set=set((tf_1.vocabulary_).keys())\n",
    "\n",
    "#             row_df=df_test[df_train[target_name] == text]\n",
    "        \n",
    "#             new_tf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "#             new_tfidf_matrix = new_tf.fit_transform(row_df[data_name])\n",
    "#             wordDict=tf.vocabulary_\n",
    "#             wordSet=set(wordDict.keys())\n",
    "\n",
    "            for word in word_set:\n",
    "                if word in wordDict.keys(): \n",
    "                    new_df[word][category]+=1\n",
    "                \n",
    "        for word in new_df:\n",
    "            new_df[word][category] += 1\n",
    "            new_df[word][category] /= (len(category_df[data_name])+len(wordDict.keys()))\n",
    "    create_time=datetime.now() - start_time\n",
    "        ################# CREATE ########################\n",
    "        \n",
    "    count=0\n",
    "    result_dict={}\n",
    "    for text in df_test[data_name]:\n",
    "        \n",
    "        row_df=df_test[df_train[target_name] == text]\n",
    "        \n",
    "#         tf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "#         tfidf_matrix = tf.fit_transform(row_df[data_name])\n",
    "#         wordDict=tf.vocabulary_\n",
    "#         wordSet=set(wordDict.keys())\n",
    "        word_set=set(text.split())\n",
    "        \n",
    "        \n",
    "        temp_result_dict={}\n",
    "        \n",
    "        \n",
    "        for category in set(df_test[target_name].values):\n",
    "            temp_result_dict[category]=1\n",
    "            for word in word_set:\n",
    "                if word in wordDict.keys():\n",
    "                    temp_result_dict[category] *= new_df[word][category]\n",
    "            temp_result_dict[category] *= P_Cj_dict[category]\n",
    "        \n",
    "        temp_max=max(temp_result_dict.values())\n",
    "        \n",
    "        for category in temp_result_dict.keys():\n",
    "            if temp_result_dict[category]==temp_max:\n",
    "                result_dict[count]=category\n",
    "        \n",
    "    return result_dict,create_time\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "#####################################################################################################################################\n",
    "def multVirt(df_train,df_test,data_name='text',target_name='label'):\n",
    "    df_train=df_train.fillna(\"\")\n",
    "    df_test=df_test.fillna(\"\")\n",
    "    start_time = datetime.now()\n",
    "    label_dict={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "    tfidf_matrix = tf.fit_transform(df_train[data_name])\n",
    "    wordDict=tf.vocabulary_\n",
    "    \n",
    "    \n",
    "    new_df=pd.DataFrame( [[float(0)]*len(wordDict.keys())]*len(set(df_train[target_name].values)), index=set(df_train[target_name].values) )\n",
    "    new_df.columns = wordDict.keys()\n",
    "    \n",
    "    P_Cj_dict={}\n",
    "    \n",
    "    for category in set(df_train[target_name].values):\n",
    "        \n",
    "        category_df=df_train[df_train[target_name] == category]\n",
    "        \n",
    "        tf_1 = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "#         tfidf_matrix = tf.fit_transform(category_df[data_name])\n",
    "#         wordDict=tf.vocabulary_\n",
    "        \n",
    "        \n",
    "\n",
    "        ################# CREATE ########################\n",
    "        \n",
    "        P_Cj_dict[category]= len(df_train[target_name].values)/len(df_train[data_name])\n",
    "        \n",
    "        for text in category_df[data_name]:\n",
    "            word_set=set(text.split())\n",
    "#             tfidf_matrix = tf_1.fit_transform(text)\n",
    "#             word_set=set((tf_1.vocabulary_).keys())\n",
    "\n",
    "#             row_df=df_test[df_train[target_name] == text]\n",
    "        \n",
    "#             new_tf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "#             new_tfidf_matrix = new_tf.fit_transform(row_df[data_name])\n",
    "#             wordDict=tf.vocabulary_\n",
    "#             wordSet=set(wordDict.keys())\n",
    "\n",
    "            for word in word_set:\n",
    "                if word in wordDict.keys(): \n",
    "                    new_df[word][category]+=1\n",
    "                \n",
    "        for word in new_df:\n",
    "            new_df[word][category] += 1\n",
    "            new_df[word][category] /= (len(category_df[data_name])+2)\n",
    "    create_time=datetime.now() - start_time\n",
    "        ################# CREATE ########################\n",
    "        \n",
    "    count=0\n",
    "    result_dict={}\n",
    "    for text in df_test[data_name]:\n",
    "        \n",
    "        row_df=df_test[df_train[target_name] == text]\n",
    "        \n",
    "#         newtf = TfidfVectorizer(analyzer='word',ngram_range=(1,1), min_df=0)\n",
    "#         tfidf_matrix = newtf.fit_transform(row_df[data_name])\n",
    "#         wordDict=newtf.vocabulary_\n",
    "#         wordSet=set(wordDict.keys())\n",
    "        word_set=set(text.split())\n",
    "        \n",
    "        \n",
    "        temp_result_dict={}\n",
    "        \n",
    "        \n",
    "        for category in set(df_test[target_name].values):\n",
    "            temp_result_dict[category]=1\n",
    "            for word in word_set:\n",
    "                if word in wordDict.keys():\n",
    "                    temp_result_dict[category] *= new_df[word][category]\n",
    "            temp_result_dict[category] *= P_Cj_dict[category]\n",
    "        \n",
    "        temp_max=max(temp_result_dict.values())\n",
    "        \n",
    "        for category in temp_result_dict.keys():\n",
    "            if temp_result_dict[category]==temp_max:\n",
    "                result_dict[count]=category\n",
    "        \n",
    "    return result_dict,create_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.misc', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data={'text': list(newsgroups_train.data) ,'label': list(newsgroups_train.target) })\n",
    "df_test = pd.DataFrame(data={'text': list(newsgroups_test.data) ,'label': list(newsgroups_test.target) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','') \n",
    "    df['text'] = df['text'].str.replace('[^a-z ]','') \n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=prep(df_train)\n",
    "df_test=prep(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       well andy familiarized ofthe current theoriesh...\n",
      "1       compute many astronomical things go get xephem...\n",
      "2       hello netters fairly weak question ask everybo...\n",
      "3                                 choose following answer\n",
      "4       looking graphic images earth shot space prefer...\n",
      "                              ...                        \n",
      "2014    lot discussion tyre sum ezekiel prophesiedthat...\n",
      "2015    cs bird may flaking expecting die soonor c may...\n",
      "2016    weiteks addressphone number id like get inform...\n",
      "2017                force people associate others willyes\n",
      "2018    sf please dont answerthat number libertarians ...\n",
      "Name: text, Length: 2019, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_test=(df_train['text']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(split_test))\n",
    "# print(split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type((np.array_split(split_test,1))[0]))\n",
    "# print(len((np.array_split(split_test,1))[0]))\n",
    "\n",
    "# print((np.array_split(split_test,1))[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_1=list((np.array_split(split_test,1))[0])\n",
    "\n",
    "# text_dict={}\n",
    "# for text in test_1:\n",
    "#     loc_word_set=set(text.split())\n",
    "#     for \n",
    "\n",
    "\n",
    "\n",
    "# print(test_1.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "<ipython-input-392-0cc414f15b95>:149: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  row_df=df_test[df_train[target_name] == text]\n"
     ]
    }
   ],
   "source": [
    "r1,t1=multVirt(df_train,df_test,data_name='text',target_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "<ipython-input-392-0cc414f15b95>:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  row_df=df_test[df_train[target_name] == text]\n"
     ]
    }
   ],
   "source": [
    "r2,t2=multNom(df_train,df_test,data_name='text',target_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data storege\n",
    "#### For bouth models:\n",
    "##### matrix[word][category] + dict_of_P[category]=val\n",
    "\n",
    "#### input dataFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2019, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Nominali:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start_time = datetime.now()\n",
    "create_time=datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:38.797088\n"
     ]
    }
   ],
   "source": [
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Virtuali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:37.464234\n"
     ]
    }
   ],
   "source": [
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "### secRate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(r,df_test_l):\n",
    "    for num in range(len(r)):\n",
    "        count=0\n",
    "        if r[num]==df_test_l[num]:\n",
    "            count+=1\n",
    "    return count/ len(df_test_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multVirt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multNom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "#### X works better for this data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
