# -*- coding: utf-8 -*-
"""ex4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ri7bpG9UVug0YaVmFfXrqnj7hhA_24Fb
"""
#Dmitry Korkin 336377429
#Shmuel Atias 300987443

import pandas as pd
import numpy as np
import string
from sklearn.model_selection import train_test_split
import io
#from google.colab import files
from sklearn import metrics
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM,GRU, Bidirectional
from keras.layers import Conv1D, Flatten, MaxPooling1D

from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

#uploaded = files.upload()
train_data = pd.read_csv("training_ex4_dl2021b.csv")
#train_data= pd.read_csv(io.StringIO(uploaded['training_ex4_dl2021b.csv'].decode('utf-8')))
#test_data = pd.read_csv("test_ex3_dl2021b.csv")
#uploaded = files.upload()
test_data = pd.read_csv("test_ex4_dl2021b.csv")
#test_data= pd.read_csv(io.StringIO(uploaded['test_ex4_dl2021b.csv'].decode('utf-8')))

sentences = train_data['sentence'].values
labels = train_data['label'].values
sentences_test=test_data['sentence'].values

x_train,x_test,y_train,y_test = train_test_split(sentences, labels,test_size=0.25,random_state=1000)

tokenizer = Tokenizer(num_words=50000, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
#tokenizer.fit_on_texts(x_train)
tokenizer.fit_on_texts(sentences)

# X_train = tokenizer.texts_to_sequences(x_train)
# X_test = tokenizer.texts_to_sequences(x_test)
X_train = tokenizer.texts_to_sequences(sentences)
X_test=tokenizer.texts_to_sequences(sentences_test)

#print(sentences)
#print(tokenizer.word_index)
#print()

vocab_size = len(tokenizer.word_index) + 1

X_train = pad_sequences(X_train, padding='post', maxlen=200)
X_test = pad_sequences(X_test, padding='post', maxlen=200)
#print(X_train)

#embedding_dim = 64 4.2
embedding_dim = 64

model = Sequential() #4.2
model.add(Embedding(vocab_size, embedding_dim, input_length=200))
model.add(Bidirectional(LSTM(64,  return_sequences=True)))
model.add(Bidirectional(LSTM(32)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) #4.2

# model = Sequential()
# model.add(Embedding(vocab_size, embedding_dim, input_length=200))
# model.add(Bidirectional(LSTM(64,  return_sequences=True)))
# model.add(Bidirectional(LSTM(32)))
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#model.fit(X_train, y_train,epochs=5,batch_size=10)
##model.fit(X_train, labels,epochs=5,batch_size=10) 4.2
model.fit(X_train, labels,epochs=5,batch_size=10)

print(model.summary())

pred=model.predict_classes(X_test)

#print(metrics.accuracy_score(y_test,pred))

df = pd.DataFrame({'id': test_data['id'],
                   'label':[int(x) for x in pred],})

#df.to_csv('out.csv',index=False)
#files.download('out.csv')

#test_loss, test_acc = model.evaluate(test_dataset)

#print('Test Loss:', test_loss)
#print('Test Accuracy:', test_acc)

'''model1 = Sequential()
model1.add(Embedding(vocab_size, embedding_dim, input_length=80))
model1.add(GRU(50,dropout=0.5))
#model.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])'''

#model1.fit(X_train, y_train,epochs=10,batch_size=10)

#pred=model1.predict_classes(X_test)

#print(metrics.accuracy_score(y_test,pred))